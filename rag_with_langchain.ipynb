{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1612fa",
   "metadata": {},
   "source": [
    "# RAG with LangChain\n",
    "\n",
    "- Step-1 : Extract the PDF text\n",
    "- Step-2 : Chunk the extracted PDF text\n",
    "- Step-3 : Create a vector store with the PDF chunks\n",
    "- Step-4 : Create a retriever which returns the relevant chunks\n",
    "- Step-5 : Build context from the relevant chunk texts\n",
    "- Step-6 : Build the RAG chain using rag prompt, LLM and string output parser.\n",
    "- Step-7 : Run the RAG chain to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18cc4d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\malte\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1478a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"OpenAI API key loaded successfully!\")\n",
    "else:\n",
    "    print(\"Error: OPENAI_API_KEY not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29dbc43",
   "metadata": {},
   "source": [
    "### Extract PDF text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5859f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Get your project's root directory (e.g., /path/to/rag_learning)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 2. Define a path for the 'content' folder\n",
    "content_dir = os.path.join(current_dir, \"content\")\n",
    "\n",
    "# 3. Create the 'content' folder if it doesn't already exist\n",
    "os.makedirs(content_dir, exist_ok=True)\n",
    "\n",
    "# 4. Define the full, correct path for the NEW PDF\n",
    "pdf_path = os.path.join(content_dir, \"sparql_query_translation.pdf\")\n",
    "\n",
    "# 5. Download the PDF file\n",
    "pdf_url = 'https://arxiv.org/pdf/2507.10045.pdf'\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "# 6. Save the file to your 'content' folder\n",
    "with open(pdf_path, 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(f\"Successfully downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7293336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def pdf_extract(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyPDFLoader.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_path (str): The file path of the PDF to be extracted.\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: A list of Document objects containing the extracted text from the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"PDF file text is extracted...\")\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pdf_text = loader.load()\n",
    "\n",
    "    return pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807394aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file text is extracted...\n"
     ]
    }
   ],
   "source": [
    "pdf_text = pdf_extract(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2ed49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents = {len(pdf_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a9244",
   "metadata": {},
   "source": [
    "### Chunk PDF text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704b32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_chunk(pdf_text: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits extracted PDF text into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_text (List[Document]): A list of Document objects containing extracted text from a PDF.\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: A list of chunked Document objects.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"PDF file text is chunked....\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(pdf_text)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76ae38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file text is chunked....\n",
      "Number of chunks = 67\n"
     ]
    }
   ],
   "source": [
    "chunks = pdf_chunk(pdf_text)\n",
    "print(f\"Number of chunks = {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3e1c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='July 2025\n",
      "Automating SPARQL Query Translations\n",
      "between DBpedia and Wikidata\n",
      "Malte Christian BARTELSa,1, Debayan BANERJEE a and Ricardo USBECK a\n",
      "a Leuphana University of Lüneburg, Lüneburg, Germany\n",
      "ORCiD ID: Malte Christian Bartels https://orcid.org/0009-0006-2113-3322, Debayan\n",
      "Banerjee https://orcid.org/0000-0001-7626-8888, Ricardo Usbeck\n",
      "https://orcid.org/0000-0002-0191-7211\n",
      "Abstract. Purpose: This paper investigates whether state-of-the-art Large Lan-\n",
      "guage Models (LLMs) can automatically translate SPARQL between popular\n",
      "Knowledge Graph (KG) schemas. We focus on translations between the DBpedia\n",
      "and Wikidata KG, and later on DBLP and OpenAlex KG. This study addresses a\n",
      "notable gap in KG interoperability research by evaluating LLM performance on\n",
      "SPARQL-to-SPARQL translation.\n",
      "Methodology: Two benchmarks are assembled, where the first aligns 100 DBpe-\n",
      "dia–Wikidata queries from QALD-9-Plus dataset; the second contains 100 DBLP' metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Malte Christian Bartels; Debayan Banerjee; Ricardo Usbeck', 'doi': 'https://doi.org/10.48550/arXiv.2507.10045', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Automating SPARQL Query Translations between DBpedia and Wikidata', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2507.10045v1', 'source': 'c:\\\\Users\\\\malte\\\\Desktop\\\\Coding\\\\rag_learning\\\\content\\\\sparql_query_translation.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a8c96",
   "metadata": {},
   "source": [
    "### Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cb0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma vector store is created...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_pdf_langchain\")\n",
    "\n",
    "def create_vector_store(chunks: List[Document], db_path: str) -> Chroma:\n",
    "    \"\"\"\n",
    "    Creates a Chroma vector store from chunked documents.\n",
    "\n",
    "    Parameters:\n",
    "    chunks (List[Document]): A list of chunked Document objects.\n",
    "    db_path (str): The directory path to persist the vector store.\n",
    "\n",
    "    Returns:\n",
    "    Chroma: A Chroma vector store containing the embedded documents.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Chroma vector store is created...\\n\")\n",
    "    # Ensure your OPENAI_API_KEY is already loaded in the environment before this runs!\n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    # Create and persist the database specifically for these chunks\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "\n",
    "    return db\n",
    "\n",
    "# Run the function with your corrected path\n",
    "db = create_vector_store(chunks, persistent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735c879",
   "metadata": {},
   "source": [
    "### Retrieve relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e61ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(db: Chroma, query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieves relevant document chunks from the Chroma vector store based on a query.\n",
    "\n",
    "    Parameters:\n",
    "    db (Chroma): The Chroma vector store containing embedded documents.\n",
    "    query (str): The query string to search for relevant document chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[Document]: A list of retrieved relevant document chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "    print(\"Relevant chunks are retrieved...\\n\")\n",
    "    relevant_chunks = retriever.invoke(query)\n",
    "\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7df0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant chunks are retrieved...\n",
      "\n",
      "Number of relevant chunks = 2\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain the paper approach in one line\"\n",
    "\n",
    "relevant_chunks = retrieve_context(db, query)\n",
    "print(f\"Number of relevant chunks = {len(relevant_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4d97c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk-0\n",
      "page_content='ran Associates, Inc.; 2022. p. 24824-37. Available from: https:\n",
      "//proceedings.neurips.cc/paper_files/paper/2022/file/\n",
      "9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.' metadata={'title': 'Automating SPARQL Query Translations between DBpedia and Wikidata', 'source': 'c:\\\\Users\\\\malte\\\\Desktop\\\\Coding\\\\rag_learning\\\\content\\\\sparql_query_translation.pdf', 'total_pages': 18, 'arxivid': 'https://arxiv.org/abs/2507.10045v1', 'page_label': '18', 'author': 'Malte Christian Bartels; Debayan Banerjee; Ricardo Usbeck', 'trapped': '/False', 'creator': 'arXiv GenPDF (tex2pdf:)', 'doi': 'https://doi.org/10.48550/arXiv.2507.10045', 'creationdate': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'page': 17, 'producer': 'pikepdf 8.15.1', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}\n",
      "\n",
      "\n",
      "Chunk-1\n",
      "page_content='ran Associates, Inc.; 2022. p. 24824-37. Available from: https:\n",
      "//proceedings.neurips.cc/paper_files/paper/2022/file/\n",
      "9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.' metadata={'creationdate': '', 'total_pages': 18, 'creator': 'arXiv GenPDF (tex2pdf:)', 'trapped': '/False', 'source': 'c:\\\\Users\\\\malte\\\\Desktop\\\\Coding\\\\rag_learning\\\\content\\\\sparql_query_translation.pdf', 'producer': 'pikepdf 8.15.1', 'doi': 'https://doi.org/10.48550/arXiv.2507.10045', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Automating SPARQL Query Translations between DBpedia and Wikidata', 'arxivid': 'https://arxiv.org/abs/2507.10045v1', 'page_label': '18', 'page': 17, 'author': 'Malte Christian Bartels; Debayan Banerjee; Ricardo Usbeck', 'license': 'http://creativecommons.org/licenses/by/4.0/'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(relevant_chunks):\n",
    "  print(f\"Chunk-{i}\")\n",
    "  print(chunk)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33392bc9",
   "metadata": {},
   "source": [
    "### Build context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fae096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(relevant_chunks: List[Document]) -> str:\n",
    "    \"\"\"\n",
    "    Builds a context string from retrieved relevant document chunks.\n",
    "\n",
    "    Parameters:\n",
    "    relevant_chunks (List[Document]): A list of retrieved relevant document chunks.\n",
    "\n",
    "    Returns:\n",
    "    str: A concatenated string containing the content of the relevant chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Context is built from relevant chunks\")\n",
    "    context = \"\\n\\n\".join([chunk.page_content for chunk in relevant_chunks])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f1bab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context is built from relevant chunks\n",
      "ran Associates, Inc.; 2022. p. 24824-37. Available from: https:\n",
      "//proceedings.neurips.cc/paper_files/paper/2022/file/\n",
      "9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\n",
      "\n",
      "ran Associates, Inc.; 2022. p. 24824-37. Available from: https:\n",
      "//proceedings.neurips.cc/paper_files/paper/2022/file/\n",
      "9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\n"
     ]
    }
   ],
   "source": [
    "context = build_context(relevant_chunks)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89f261",
   "metadata": {},
   "source": [
    "### Combine all the steps into one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1260c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def get_context(inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Creates or loads a vector store for a given PDF file and extracts relevant chunks based on a query.\n",
    "\n",
    "    Args:\n",
    "        inputs (Dict[str, str]): A dictionary containing the following keys:\n",
    "            - 'pdf_path' (str): Path to the PDF file.\n",
    "            - 'query' (str): The user query.\n",
    "            - 'db_path' (str): Path to the vector database.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary containing:\n",
    "            - 'context' (str): Extracted relevant context.\n",
    "            - 'query' (str): The user query.\n",
    "    \"\"\"\n",
    "    pdf_path, query, db_path  = inputs['pdf_path'], inputs['query'], inputs['db_path']\n",
    "\n",
    "    # Create new vector store if it does not exist\n",
    "    if not os.path.exists(db_path):\n",
    "        print(\"Creating a new vector store...\\n\")\n",
    "        pdf_text = pdf_extract(pdf_path)\n",
    "        chunks = pdf_chunk(pdf_text)\n",
    "        db = create_vector_store(chunks, db_path)\n",
    "\n",
    "    # Load the existing vector store\n",
    "    else:\n",
    "        print(\"Loading the existing vector store\\n\")\n",
    "        embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        db = Chroma(persist_directory=db_path, embedding_function=embedding_model)\n",
    "\n",
    "    relevant_chunks = retrieve_context(db, query)\n",
    "    context = build_context(relevant_chunks)\n",
    "\n",
    "    return {'context': context, 'query': query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38056972",
   "metadata": {},
   "source": [
    "### Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8a8d64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" You are an AI model trained for question answering. You should answer the\n",
    "  given question based on the given context only.\n",
    "  Question : {query}\n",
    "  \\n\n",
    "  Context : {context}\n",
    "  \\n\n",
    "  If the answer is not present in the given context, respond as: The answer to this question is not available\n",
    "  in the provided content.\n",
    "  \"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableLambda(get_context)\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | str_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3eb7c4",
   "metadata": {},
   "source": [
    "### Run RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "987277c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the chroma DB path\n",
    "current_dir = \"/content/rag\"\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_pdf_langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a438bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PDF file\n",
    "import requests\n",
    "\n",
    "pdf_url = 'https://arxiv.org/pdf/2507.10045.pdf'\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "pdf_path = 'content/sparql_query_translation.pdf'\n",
    "with open(pdf_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73082e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query\n",
    "query = 'What was zero-shot learning used for in the paper?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6adb5193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the existing vector store\n",
      "\n",
      "Relevant chunks are retrieved...\n",
      "\n",
      "Context is built from relevant chunks\n",
      "Query:What was zero-shot learning used for in the paper?\n",
      "\n",
      "Generated answer:Zero-shot learning was used in the paper to mitigate baseline limitations, particularly by including an entity-relation mapping variable to enhance the zero-shot prompt. This approach was aimed at directly quantifying the impact of schema alignment information when applied to the models Llama 3.1-8B and Mistral-Large-Instruct-2407.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke({'pdf_path':pdf_path, 'query':query, 'db_path':persistent_directory})\n",
    "print(f\"Query:{query}\\n\")\n",
    "print(f\"Generated answer:{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
